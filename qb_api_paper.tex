
\documentclass{llncs}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{multirow}
\usepackage{tabularx}

\begin{document}

\title{Facilitating the exploitation of Linked Open Statistical Data: JSON-QB API requirements and design criteria}

\author{xxx ddd \and yyy sss}

\maketitle

\begin{abstract}

Nowadays, statistical open data become more and more popular while applications using them are increasingly created. Linked open statistical data (LOSD) are the best practice to publish data as they facilitate the integration across portals. Although there are many tools using LOSD technologies, nothing succeeds in wide exploitation. Technical knowledge and high skills are necessary for the creation of LOSD tools. Moreover, different publish practices are being used, hampering data interoperability and leading to data software silos. In this paper we present the JSON-QB API which uses more widespread technologies. The requirements, the design criteria and a proof of concept of API implementation are also presented.  

\end{abstract}

\section{Introduction}\label{sec:intro}

Recently, many governments, organisations and companies are opening up their data for others to reuse through \textit{Open Data} portals  \cite{Kalampokis:2011:IJWET}. These data can be exploited to create added value services, which can increase transparency, contribute to economic growth and provide social value to citizens \cite{Janssen:2012}.

A major part of open data concerns statistics (e.g. economical and social indicators) \cite{Capadisli:2013}. These data are often organised in a multidimensional way, where a measured fact is described based on a number of dimensions. In this case, statistical data are compared to data cubes. Thus, we onwards refer to statistical multidimensional data as \textit{data cubes} or just \textit{cubes}.

Linked data has been introduced as a promising paradigm for opening up data because it facilitates data integration on the Web \cite{Bizer:2009}. Concerning statistical data, standard vocabularies such as the RDF data cube (QB) vocabulary\cite{Cyganiak:2014:W3C}, SKOS\cite{Miles:2009:W3C} and XKOS\cite{XKOS} enable modelling data cubes as Linked Open Statistical Data (LOSD). In this way they facilitate data integration across portals that adopt these technologies and enable the creation of added value services and applications on top of them.

Although LOSD potential is high, their exploitation is currently low for two reasons. First, using LOSD requires skills and tooling (e.g. RDF, SPARQL) that are less widespread than some other web technologies (e.g. JSON, REST). For example, there are many visualization libraries that consume data in JSON format (e.g D3.js, charts.js), while there are just a few that consume RDF and their functionality is limited. 

Second, many portals that use the standard vocabularies (QB, SKOS, XKOS) often adopt different publishing practices \cite{KalampokisChallenges}, thus hampering their interoperability. As a result it is not easy to create generic software tools that can be reused across LOSD. Usually, developed tools assume that data are published only in a specific form.

In order to unleash the full potential of LOSD there is a need to standardize the interaction with LOSD and hide most of the complexity. Towards this end, in this paper we describe the requirements and design criteria of an API that standardizes the interaction (i.e. input, output and functionality) with LOSD in a way that facilitates the development of reusable software. The API aims to exploit the advantages of linked data (e.g. easy data integration) while making data available in a structure and format that is familiar to a larger group of developers. Some of the flexibility, and associated complexity, of linked data is removed, in favour of simplicity and ease of use. Specifically, the API aims to support developers use LOSD stored in the form of an RDF Data Cube, while assuming minimal knowledge of linked data technologies. Moreover, it offers a uniform way to access the underlying data by hiding any data discrepancies, thus enabling the development of generic software tools that can be reused across datasets. 

The rest of the paper is organized as follows ... +++ 

\section{Motivation}\label{sec:motivation}

Currently, many LOSD have been made available on the Web through official portals launched by private or public bodies that own the data. For example, the European Commission's Digital Agenda provides its Scoreboard\footnote{\url{http://digital-agenda-data.eu/data}} as LOSD. Census data of 2011 from Ireland\footnote{\url{http://data.cso.ie}} and Italy\footnote{\url{http://datiopen.istat.it}} have been published as linked data by their National Statistics Institutes. The Department for Communities and Local Government (DCLG)\footnote{\url{http://opendatacommunities.org/data }} in the UK, the Scottish Government\footnote{\url{http://statistics.gov.scot}}, and the Statistics Bureau of Japan\footnote{\url{http://data.e-stat.go.jp}} opened up their statistics as linked data. 

Although the above portals use the same standard vocabularies to publish LOSD, they often adopt different publishing practices. As a result, generic tools that operates across LOSD datasets cannot be created. However, tools for exploiting LOSD have already been developed which assume data published only in a specific way. For example, existing tools enable: i) the browsing of LOSD e.g. OpenCube Browser\cite{Kalampokis:2014}, CODE Query wizard\cite{CODE:2014} ii) the performance of OLAP operations like roll-up/drill-down, slice, dice e.g. OpenCube OLAP Browser \cite{KalampokisIAOS}, QB2OLAP\cite{QB2OLAP} iii) the performance of  statistical analysis on LOSD e.g. OpenCube R statistical analysis tool\cite{Kalampokis:2014} and iv) the visualization of LOSD e.g. CubeViz\cite{Cubeviz}, StatSpace\cite{Statspace}.

Except from the exploitation tools, complete platforms (e.g. PublishMyData\footnote{http://www.swirrl.com/}) are developed that aim both in to publishing and exploiting LOSD. In this case, published data can be consumed only by tools of the same platform, since different publishing practices are adopted. This lead to the creation of LOSD system silos (software and data) that cannot interoperate among each other.

All the above tools and platforms follow the same traditional architecture (figure \ref{fig:asis}) where each tool has an integrated access layer. If several tools are created for the same portal (i.e. same publishing practices), then each tool has to develop separately  a similar data access layer. In addition, if a tool has to be used at another portal, then a new data access layer has to be created leading to additional costs. More importantly, the development of data access layers requires significant programming expertise in LOSD, a skill that is not widely available between developers.

As a result, there is a need to standardize the interaction with LOSD in a way that hides the LOSD complexity to the developers and offers a uniform way to access the data hiding any data discrepancies. 

\begin{figure}
\begin{center}
  \includegraphics[width=110mm]{images/asis.jpg}
  \end{center}
\caption{Traditional architecture for LOSD tools}
\label{fig:asis}
\end{figure}

\section{Methodology}\label{sec:methodology}

In order to achieve the objectives of the paper we adopt the following methodology:
\begin{itemize}
\item Study the related work. We focus on: i) APIs that facilitate the interaction with data cubes and ii) data formats that can be exploited to represent the result (i.e. output) of the API. 
\item Collect user requirements from developers that currently create applications for LOSD. 
\end{itemize}

Currently, there exist many APIs that facilitate the interaction with data cubes. These APIs offer basic functionality that covers the cube's logical model, but they also support more advanced OLAP operations including aggregations, slicing, roll-up/drill-down etc. For example, the Oracle OLAP Java API \cite{ORACLEAPI} allows users to select, explore, aggregate, calculate, and perform other analytical tasks on data stored in an Oracle data warehouse. Olap4j\footnote{http://www.olap4j.org} is another Java API for accessing data cubes, which is compatible with many OLAP servers (e.g. Mondrian, Palo and SAP Business Warehouse). It enables the browsing of meta-data including the cubes, dimensions and  hierarchies. Olap4j also supports Multidimensional Expressions (MDX) that is the query language for OLAP.

There are also some REST APIs with similar functionality. The Data Brewery\footnote{http://databrewery.org/} offers a set of Python tools, including a REST API, for processing and analysing data cubes stored at a relational data base (e.g. MySQL, PostgreSQL). Apache Lens\footnote{lens.apache.org} is an analytics platform that integrates Hadoop with traditional data warehouses (e.g. Apache Hive, Amazon Redshift). Lens provides a REST API to handle data cubes, that also supports ``OLAP Cube QL" - a high level SQL like language to query data organized as data cubes.

All the above APIs handle data cubes that are stored in traditional databases or data-warehouse. However, none of them can handle data cubes  stored as linked data using the QB vocabulary.  

Regarding the output of the REST APIs, JSON is a commonly used simple format. Existing REST APIs use 
case-specific JSON responses, however JSON extension formats have already been proposed to model data cubes and linked data. Specifically, JSON-LD\footnote{https://json-ld.org} offers a method for encoding linked data using JSON. While, JSON-stat\footnote{https://json-stat.org/} is a JSON format for modelling linked statistical data, however the structure is too complicated when it comes to simple visualizations (e.g. maps). Moreover, it has some limitations e.g. does not support pagination of results. 

Finally, to collect the user requirements, we established a continuous discussion with developers that currently create applications for LOSD. The discussion mainly occurs within the EU funded project OpenGovIntelligence\footnote{http://www.opengovintelligence.eu/}, that aims to exploit LOSD for improving the public services. To facilitate the collection of requirements we organized a dedicated workshop in Manchester with participation of relevant developers. 


\section{Requirements and design criteria}\label{sec:reqs}

This section presents the requirements and design criteria related to the JSON-QB API. Generally, the API should follow patterns and practices familiar to ``mainstream" web developers, which facilitate the creation of visualisations and interactive applications that use the data. Moreover, it should be suitable for use by a wide range of statistics publishing organisations, so that data users can have a standard interface to LOSD. This will put constraints on the way that publishers manage their data, however those constraints should be reasonable and manageable. 
%A summary of the requirements is presented at table \ref{tbl:req}.
%
%\begin{table}
%\caption{Summary of JSON-QB API requirements}
%\begin{tabular}{p{2cm}p{4cm}p{5.9cm}}
%\hline\noalign{\smallskip}
%\textbf{Type} & \textbf{Name} & \textbf{Description}\\
%\noalign{\smallskip}
%\hline
%\noalign{\smallskip}
%\multirow{3}{*}{functional} & Search data cubes & Search for data cubes at linked statistical data portals based on specified criteria\\\noalign{\smallskip}
% & Explore data cube structure & Having a cube at hand, get information about its structure (e.g. dimensions, measures, attributes)\\\noalign{\smallskip}
% & Slicing and filtering & Get a sub-set (slice or dice) of cube observations based on user-defined criteria\\\noalign{\smallskip}\hline
%\multirow{4}{*}{non-functional} & Ease of use & Provide a style of interaction that is familiar to web developers\\\noalign{\smallskip}
% & Uniform data access & Hide any discrepancies of published data cubes\\\noalign{\smallskip}
% & High performance & Provide responses fast even for demanding requests\\\noalign{\smallskip}
% & Extensibility & Enable the addition of new or the modification of existing functionality\\\noalign{\smallskip}
%\hline
%\end{tabular}
%\label{tbl:req}
%\end{table}

\subsection{Search data cubes}\label{sec:search}

The linked data web currently contains many published data cubes and their number still increases. Thus, applications need to search for cubes based on some criteria. For example, get cubes that measure unemployment, or get cubes for Greece. The search can be even more complex e.g. get cubes about unemployment in Greece after 2010. To fully exploit the cube searching functionality, the JSON-QB API ideally should search over the linked data web and not be limited to a single RDF store.

The search functionality can also be extended to support not only user specific criteria (as the previous examples), but also support the ``automatic" search of compatible cubes that could be processed together. For example, having a cube at hand search for other cubes that are compatible for combined statistical analysis, for visualisation or for browsing. The cube compatibility criteria is still an open issue and is out of the scope of this paper. 

\subsection{Explore data cube structure}

Once a cube has been identified (e.g. through the search functionality) the processing application (e.g. cube browser) needs to initialize the user interface or the analysis with information related to the cube structure. For example, populate drop-down menus with the cube dimensions and measures. The QB vocabulary clearly identifies the main elements of the structure that should be accessed through the JSON-QB API:
\begin{itemize}
\item Dataset meta-data. They include information like the label, description, issue date, publisher and license.
\item Dimensions. They include all the dimension properties of the cube (e.g. reference area, reference period).
\item Measures. They include all the measure properties of the cube (e.g. unemployment, poverty)
\item Attributes. They include all the attribute properties of the cube (e.g. unit of measure)
\item Dimension values. They include all the values of a dimension (e.g. male, female) that appear at the cube. 
\item Dimension levels. In the case of hierarchical data, dimension values are organized to hierarchical levels (e.g. region, district).
\item Attribute values. They include all the values of an attribute (e.g. euro, dollar) that appear at the cube. 
\end{itemize} 

Regarding the last three elements, the QB vocabulary does not offer a way to retrieve the values / levels directly from the structure. Thus the API should iterate over the cube observations, which is a time consuming task.

\subsection{Slicing and filtering}\label{sec:slice}

There are already methods available for downloading entire data cubes but people often want just small parts.  Whole cubes are often too big to be well-suited to interactive applications, and if the data updates frequently,  then it's important for people to be able to retrieve up-to-date extracts of the data, rather than keeping their own copies of full datasets up to date. The JSON-QB API should allow applications to take exactly the data they want by defining constrains (i.e. filters) to the dimension values. It should support many filtering options including:
\begin{itemize}
\item Single values e.g. refPeriod=2010.
\item Multiple values e.g. refPeriod=[2010, 2011, 2012]
\item Ranges e.g refPeriod=[2010 ... 2015]
\item Greater/smaller than e.g. refPeriod$>$2010
\item Hierarchical data filtering e.g. refArea=``all council areas in Scotland"
\end{itemize}

In many cases, applications do not need all the requested data at once, because they process them at bunches. For example, a cube browser shows a part of the data allowing the user to navigate to the previous/next page of data. Thus, the JSON-QB API should support paging and ordering of the results. The ordering of the results can be in ascending or descending order based on a dimension. However, in some cases the ordering of the results may be complicated for example ordering based on 2 ore more dimension. Moreover, lexicographical ordering is not always appropriate (e.g. for the days of the week), thus other types of ordering should be applied or the ordering of results based on multiple dimensions

\subsection{Ease of use}

Linked data offer many benefits to web developers, including the easy integration on the web. However, linked data technologies (i.e. RDF, SPARQL) are unfamiliar to many developers, thus creating many obstacles at their adoption. The purpose of the JSON-QB API is to exploit the advantages of linked data through a style of interaction that is familiar to web developers, thus helping them create data visualisations and applications. It is not necessary for the API to be a complete ``round-trippable" representation of the data, it is acceptable to lose some information in favour of greater ease of use

The ease of use of an API is related both to the input of the API (API calls) and the output. Regarding the input of the API there are mainly two design options: i) use a separate parameter for each required input and ii) model all the required input as a JSON object. The first option is most popular at existing APIs, while the second is more flexible and expressive. For example, it enables the expression of relations other than equality e.g. greater than (see Table \ref{tbl:apiinput}).

\begin{table}
\caption{Input of API: separate parameters vs JSON object}
\begin{tabular}{ll}
\hline\noalign{\smallskip}
\textbf{Separate parameters} & \textbf{Input as JSON}\\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
\begin{minipage}[t]{2.2in}
 \begin{verbatim} 
GET /slice?dataset=home-care&
            gender=male         
\end{verbatim}
\end{minipage}
&
 \begin{minipage}[t]{2.3in}
\begin{verbatim} 
{"dataset": "home-care",
 "filter": {
   "gender": "male"
}}
\end{verbatim}
\end{minipage}\\\noalign{\smallskip}
\begin{minipage}[t]{2.2in}
 \begin{verbatim} 
Cannot be expressed                    
\end{verbatim}
\end{minipage}
&
\begin{minipage}[t]{2.3in}
\begin{verbatim} 
{"dataset": "home-care",
 "filter": {
   "gender": "male"
   "age": { "greater-than": 50}
}}
\end{verbatim}
\end{minipage}\\\noalign{\smallskip}
\hline
\end{tabular}
\label{tbl:apiinput}
\end{table}


Regarding the output of the API, JSON is a popular, easy to use format. Usually, applications and visualizations do not require an n-array/ tabular response (e.g. JSON-stat); an array of observations is sufficient and more straightforward. In case that a tabular response is required, then it can easily be constructed from the observations. 
%An example of a JSON code that could represents an array of observations is presented below.

%\begin{verbatim} 
%{ "observations": [ 
%    {"gender"       : "male", 
%     "age"          : "55-65", 
%     "refPeriod"    : "2016",
%     "refArea"      : "Scotland",
%     "unemployment" : "4.3",
%     "unit"         : "percent"}, 
%    {"gender"       : "female", 
%     "age"          : "55-65", 
%     "refPeriod"    : "2016",
%     "refArea"      : "Scotland",
%     "unemployment" : "4.7",
%     "unit"         : "percent"},
%     ...
%]}
%\end{verbatim}

While JSON-QB API aims in hiding some of the complexity of linked data, responses should include URIs as identifiers of key entities, to retain the connection to data on the web and to support reliable combining of data from different sources within a data consuming application.

\subsection{Uniform data access}

Currently many LOSD have been published, however a lot of them adopt different publishing practices. The JSON-QB API should work on top of any of these data, offering uniform access to the data and hide any discrepancies. Obviously, this will require separate implementations to comply with the different publishing practices. 

Ideally, the standardization of the JSON-QB API specification will also contribute to the formulation of an application profile for the QB vocabulary. The profile will include best practices that can be used by data publishers to provide data in a compatible way, facilitating in this way the development of generic LOSD tools. This will add some constraints on the way that publishers manage their data, however those constraints will lead to greater exploitation of the data. 

\begin{figure}[h!]
  \includegraphics[width=110mm]{images/overview.jpg}
\caption{Solution overview}
\label{fig:overview}
\end{figure}

\subsection{High performance}

The volume of data published as linked data cubes is big, reaching the magnitude of million triples per cube. Thus, SPARQL queries that iterate over all the observations tend to be slow. For example, a query to get all the dimension values that appear at a cube needs to iterate over all the observations. As a result, applications that use such queries seem to be non-responsive. 

The JSON-QB API can improve performance of demanding SPARQL queries through efficient caching of the responses. The caching policy (e.g. Least Recently Used, Least Frequently Used) plays an important role at the performance improvement. Note that caching of API responses is much easier that caching of arbitrary SPARQL queries. Allowing an arbitrary SPARQL query to run on a collection of data means that if any of the data changes, it is possible that the query response changes.  It is complex to analyse which queries touch a particular data cube or particular part of the data, thus making cache clearing difficult.  With the API call, most requests will return data from individual data cubes, so it is easier to know which cached responses must be invalidated when data is updated.

Another task that can improve the performance of the API is the pre-computation of aggregations: i) across a dimension of the cube e.g. compute the SUM of the sales over time and thus ignore the time dimension of the cube and ii) across a hierarchy e.g. if a cube contains the election results at municipality level, then aggregations can be computed at region and at country level. The pre-computation of the aggregations facilitates the execution of queries, because there in not the need to compute the aggregations on-the-fly when requested. 



\subsection{Extensibility}

Finally, the JSON-QB API should be extensible, thus take future growth into consideration minimizing the effort required for the extension. Extensions can be implemented through the :
\begin{itemize}
\item addition of new functionality e.g. while the initial aim is to build an API on top of RDF databases, other kinds of database could be used, enabling flexibility and innovation
\item modification of existing functionality e.g. support modified search (see section \ref{sec:search}) of filtering options (see section \ref{sec:slice}).
\end{itemize} 

\begin{table}
\caption{Example API calls}
\begin{tabular}{|l|l|}
\hline
\multicolumn{2}{|c|}{\textbf{Get cube Dimensions}}\\

\hline
API call &
\begin{minipage}[t]{4.5in}
 \begin{verbatim} 
GET /dimensions?dataset=http://example.com/cube/unemployment         
\end{verbatim}
\end{minipage}\\\hline

SPARQL query  &
\begin{minipage}[t]{4.5in}
 \begin{verbatim} 
PREFIX qb: http://purl.org/linked-data/cube#
PREFIX rdfs: http://www.w3.org/2000/01/rdf-schema#
PREFIX ex: http://example.com/cube/
select distinct ?dim ?label where {
    ex:unemployment qb:structure ?dsd.
    ?dsd qb:component ?comp.
    ?comp qb:dimension ?dim.
    ?dim rdfs:label ?label.} 
\end{verbatim}
\end{minipage}\\ \hline

JSON result &
\begin{minipage}[t]{4.5in}
 \begin{verbatim} 
{"dimensions":[
  {"@id":  "http://example.com/dimension/refArea",
   "label":"Reference area"},
  {"@id":  "http://example.com/dimension/refPeriod",
   "label":"Reference period"},
  {"@id":  "http://example.com/dimension/ageGroup",
   "label":"Age group"},
 ]
}  
\end{verbatim}
\end{minipage}\\\hline

\multicolumn{2}{|c|}{\textbf{Get a cube Slice}}\\

\hline
API call &
\begin{minipage}[t]{4.5in}
 \begin{verbatim} 
GET /slice?dataset=http://example.com/cube/unemployment&
           http://example.com/dimension/refPeriod=2016
\end{verbatim}
\end{minipage}\\\hline

SPARQL query  &
\begin{minipage}[t]{4.5in}
 \begin{verbatim} 
PREFIX qb: http://purl.org/linked-data/cube#
PREFIX ex: http://example.com/cube/
select ?obs ?refArea ?ageGroup ?unemployment where {
       ?obs qb:dataSet ex:unemployment.
       ?obs ex:refPeriod "2016".     
       ?obs ex:refArea ?refArea.
       ?obs ex:ageGroup ?ageGroup.
       ?obs ex:unemploymentRate ?unemployment.
}
\end{verbatim}
\end{minipage}\\ \hline

JSON result &
\begin{minipage}[t]{4.5in}
 \begin{verbatim} 
{"observations":[
      {"@id":"http://example.com/observation/1",
        "refArea"         : "Greece",
        "ageGroup"        : "25-34",
        "unemploymentRate": 0.34
       },
       {"@id":"http://example.com/observation/2",
        "refArea"         : "Greece",
        "ageGroup"        : "35-44",
        "unemploymentRate": 0.29
       },.... ]
}     
\end{verbatim}
\end{minipage}\\\hline


\end{tabular}
\label{tbl:apiexamplecall}
\end{table}


\section{Solution}\label{sec:solution}

The architecture of the JSON-QB API (figure \ref{fig:overview}) is simple, it is developed as a middle-ware between LOSD and the applications that consume the data. It receives the API REST calls and translates them to SPARQL queries that are executed at LOSD portals. Then, the returned result are transforms to a JSON representation that can easily be consumed by applications, offering in this way uniform data access and abolishing the need to implement different data access layers for each application. Table \ref{tbl:apiexamplecall} presents some example API calls, the corresponding SPARQL queries and the returned JSON results.


We have developed an implementation of the JSON-QB API\footnote{https://github.com/OpenGovIntelligence/json-qb-api-implementation} that can be installed on top of any RDF repository that stores data using the QB vocabulary, assuming that data follow a set of common publishing practices. The implementation uses existing technologies including the Jersey framework\footnote{https://jersey.github.io/} for the implementation of the RESTful services, the Rdf4j\footnote{http://rdf4j.org/} Java framework for processing RDF data and the Gson\footnote{https://github.com/google/gson} Java library to serialize Java Objects into their JSON representation. The JSON-QB API implementation is still under development and the current version demonstrates only a subset of the proposed functionality.


\begin{figure}[h!]
  \includegraphics[width=110mm]{images/cubeVisualizer.png}
\caption{Cube Visualizer}
\label{fig:cubeVisualizer}
\end{figure}

In order to demonstrate the use of the JSON-QB API, we developed the ``Cube Visualizer" (figure \ref{fig:cubeVisualizer}), a proof of concept application that consume data produced by the JSON-QB API to presents a graphical representations of LOSD. Userâ€™s choices (e.g. measure and dimension to visualise) are translated to appropriate API calls. The returned data are presented to the user in the form of a chart e.g. bar chart, pie chart, area chart.


\section{Conclusion}\label{sec:conclusion}

We should note that while our objective is to provide an API on top of a triple store holding linked data, the same API could be implemented with other approaches to storing data






\vskip 0.5cm
\noindent \textbf{Acknowledgments.} Part of this work was funded by the European Commission
within the H2020 Programme in the context of the OpenGovIntelligence
project (http://OpenGovIntelligence.eu) under grant agreement no. 693849.

\bibliographystyle{splncs03}


\bibliography{qbbibfile}   % name your BibTeX data base


\end{document}


