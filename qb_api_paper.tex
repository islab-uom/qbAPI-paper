\documentclass{llncs}
\usepackage{graphicx}
\usepackage{listings}

\begin{document}

\title{Linked Open Statistical Data API: requirements and design criteria}

\author{xxx ddd \and yyy sss}

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}\label{sec:intro}

Recently, many governments, organisations and companies are opening up their data for others to reuse through \textit{Open Data} portals  \cite{Kalampokis:2011:IJWET}. These data can be exploited to create added value services, which can increase transparency, contribute to economic growth and provide social value to citizens \cite{Janssen:2012}.

A major part of open data concerns statistics (e.g. economical and social indicators) \cite{Capadisli:2013}. These data are often organised in a multidimensional way, where a measured fact is described based on a number of dimensions. In this case, statistical data are compared to data cubes. Thus, we onwards refer to statistical multidimensional data as \textit{data cubes} or just \textit{cubes}.

Linked data has been introduced as a promising paradigm for opening up data because it facilitates data integration on the Web \cite{Bizer:2009}. Concerning statistical data, the RDF data cube (QB) vocabulary enables modelling data cubes as linked data \cite{Cyganiak:2014:W3C}. In this way it facilitates their integration. Data provided using the QB vocabulary can be accessed using the existing machinery of Linked Data. However, skills and tooling for use of linked data (e.g. RDF, SPARQL) are less widespread than some other web technologies (e.g. JSON, REST). For example, there are many visualization libraries that consume data in JSON format (e.g D3.js, charts.js), while there are just a few that consume RDF and their functionality is limited. That's one of the reasons that there are limited application (e.g. visualisation) which exploit linked data cubes [REF???].

Moreover, many portals that use the QB vocabulary often adopt different publishing practices \cite{KalampokisChallenges}, thus hampering their interoperability. As a result it is not easy to create generic software tools that operate across linked data cubes. Usually, case specific software are created which assume that linked data cubes are published only in a specific form. 

In this paper we describe the requirements and design criteria of an API that standardizes the interaction (i.e. input and output) with linked data cubes (aka Linked Open Statistical Data) in a way that facilitates the development of generic software. The API aims to exploit the advantages of linked data (e.g. easy data integration) but hide all the complexity. Specifically, it supports developers use linked statistical data stored in the form of an RDF Data Cube, while assuming minimal knowledge of linked data technologies. Moreover, the API offers a uniform way to access the underlying data by hiding any data discrepancies, thus enabling the development of generic software tools that operate across datasets. 

The rest of the paper is organized as follows ... +++

\section{Methodology}\label{sec:methodology}

In order to achieve the objectives of the paper we adopt the following methodology:
\begin{itemize}
\item Study the related work. We focus on: i) APIs that facilitate the interaction with data cubes and ii) data formats that can be exploited to represent the result (i.e. output) of the API. 
\item Collect user requirements from developers that currently create applications for linked data cubes. 
\end{itemize}

Currently, there exist many APIs that facilitate the interaction with data cubes. These APIs offer basic functionality that covers the cube's logical model, but they also support more advanced OLAP operations including aggregations, slicing, roll-up/drill-down etc. For example, the Oracle OLAP Java API \cite{ORACLEAPI} allows users to select, explore, aggregate, calculate, and perform other analytical tasks on data stored at Oracle data warehouse. Olap4j\footnote{http://www.olap4j.org} is another Java API for accessing data cubes, which is compatible with many OLAP servers (e.g. Mondrian, Palo and SAP Business Warehouse). It enables the browsing of meta-data including the cubes, dimensions, hierarchies and members in an schema. Olap4j also supports Multidimensional Expressions (MDX) that is the query language for OLAP.

There are also some REST APIs with similar functionality. The Data Brewery\footnote{http://databrewery.org/} offers a set of Python tools, including a REST API, for processing and analysing data cubes stored at a relational data base (e.g. MySQL, PostgreSQL). Apache Lens\footnote{lens.apache.org} is an analytics platform that integrates Hadoop with traditional data warehouses (e.g. Apache Hive, Amazon Redshift). Lens provides a REST API to handle data cubes, that also supports ``OLAP Cube QL" - a high level SQL like language to query data organized as data cubes.

All the above APIs handle data cubes that are stored at traditional databases or data-warehouse. However, none of them can handle data cubes  stored as linked data using the QB vocabulary.  

Regarding the output of the REST APIs, JSON is a commonly used simple format. Existing REST APIs use 
case-specific JSON responses, however JSON extension formats have already been proposed to model data cubes and linked data. Specifically, JSON-LD\footnote{https://json-ld.org} offers a method for encoding linked data using JSON. While, JSON-stat\footnote{https://json-stat.org/} is a JSON format for modelling statistical data (i.e. data cubes), however the structure is too complicated when it comes to simple visualizations (e.g. maps). Moreover, it has some limitations e.g. does not support pagination of results. 

Finally, to collect the user requirements, we established a continuous discussion with developers that currently create applications for linked data cubes. The discussion mainly occurs within the EU funded project OpenGovIntelligence\footnote{http://www.opengovintelligence.eu/}, that aims to exploit linked data cubes for improving the public services. To facilitate the collection of requirements we organized a dedicated workshop at Manchester with participation of relevant developers. 


\section{Solution overview}\label{sec:overview}

The JSON-QB API purpose is to help users retrieve information to support visualizations and other applications. It can be easily done as its implementation fills the gap of a generic software interaction with every type of LOSD. Moreover, it retains the benefits of Linked Data such as data representation and integration, but hides most of the complexity of that for the majority of users. Next, we present an overview of the solution that was designed.

The architecture of the API is relatively simple as it is developed on the RDF Data Cube vocabulary and SPARQL using statistical data from the data cube structure. The implementation of JSON-API abolishes the need to implement different data access layers for each tool is created. In the traditional architecture data access layers had to be coded separately leading to additional costs. The JSON-API can be installed on top of any RDF repository and offers basic and advanced operations on RDF Data cubes while other kinds of database could be used, enabling flexibility and innovation.

Linked data is a good approach for standards-based publication of statistics on the web, but RDF and SPARQL are unfamiliar to many users. There are obstacles to uptake of this technology because it is perceived to be complicated. The aim of the API is to support a style of interaction that is familiar to web developers - delivering data in JSON format and using familiar styles of API call. Through SPARQL queries, JSON-API has full access at data cubes returning the asked data in the re-used format of JSON. Users can use JSON as input to the API for -get requests and receive data as output still in JSON format.

In addition, we want to standardise the API specification and try to get broad support for it, so that many data publishers can provide data in a compatible way, making tools interoperable. As well as making data easier to consume, using the API as the main method of delivering machine readable extracts of data would remove or greatly reduce the need for data publishers to provide public SPARQL endpoints. By this way cost can be reduced and reliability of open services could be improved.

\begin{figure}[h!]
  \includegraphics[width=110mm]{images/overview.jpg}
\caption{Solution overview}
\label{fig:overview}
\end{figure}


\section{Requirements and design criteria}\label{sec:reqs}

This section presents the requirements and design criteria related to the Linked Open Statistical Data API.

\subsection{Search data cubes}

The linked data web currently contains many published data cubes and their number still increases. Thus, applications need to search for cubes based on some criteria. For example, get cubes that measure unemployment, or get cubes for Greece. The search can be even more complex e.g. get cubes about unemployment in Greece after 2010. To fully exploit the cube searching functionality, the API ideally should search over the linked data web and not be limited to a single RDF store.

The search functionality can also be extended to support not only user specific criteria (as the previous examples), but also support the ``automatic" search of compatible cubes that could be processed together. For example, having a cube at hand search for other cubes that are compatible for combined statistical analysis, for visualisation of for browsing. The cube compatibility criteria is still an open issue and is out of the scope of this paper. 

\subsection{Explore data cube structure}

Once a cube has been identified (e.g. through the search functionality) the processing application (e.g. cube browser) needs to initialize the user interface or the analysis with information related to the cube structure. For example, populate drop-down menus with the cube dimensions and measures. The QB vocabulary clearly identifies the main elements of the structure:
\begin{itemize}
\item Dataset meta-data. They include information like the label, description, date issued, publisher and license.
\item Dimensions. They include all the dimension properties of the cube (e.g. reference area, reference period).
\item Measures. They include all the measure properties of the cube (e.g. unemployment, poverty)
\item Attributes. They include all the attribute properties of the cube (e.g. unit of measure)
\item Dimension values. They include all the values of a dimension (e.g. male, female) that appear at the cube. 
\item Dimension levels. In the case of hierarchical data, dimension values are organized to hierarchical levels (e.g. region, district).
\item Attribute values. They include all the values of an attribute (e.g. euro, dollar) that appear at the cube. 
\end{itemize} 

Regarding the last three elements, the QB vocabulary does not offer a way to retrieve the values / levels directly from the structure, so the API should iterate over the cube observations that is a time consuming task.

\subsection{Slicing or filtering}

There are already methods available for downloading entire data cubes but people often want just small parts.  Whole cubes are often too big to be well-suited to interactive applications, and if the data updates frequently,  then it's important for people to be able to retrieve up-to-date extracts of the data, rather than keeping their own copies of full datasets up to date. The API should allow applications to take exactly the data they want by defining constrains (i.e. filters) to the dimension values. The API should support many filtering options including:
\begin{itemize}
\item Single values e.g. refPeriod=2010.
\item Multiple values e.g. refPeriod=[2010, 2011, 2012]
\item Ranges e.g refPeriod=[2010 ... 2015]
\item Greater/smaller than e.g. refPeriod$>$2010
\item Hierarchical data filtering e.g. refArea=``all council areas in Scotland"
\item +++
\end{itemize}

In many cases, applications do not need all the requested data at once, because they process them at bunches. For example, a cube browser shows a part of the data allowing the user to navigate to the previous/next page of data. Thus, the API should support paging and ordering of the results. The ordering of the results can be in ascending or descending order based on one or more dimensions. In some cases, lexicographical ordering is not appropriate (e.g. for the days of the week), thus other types of ordering should be applied.

\subsection{Easy of use}

Linked data offer many benefits to web developers, including the easy integration on the web. However, linked data technologies (i.e. RDF, SPARQL) are unfamiliar to many developers, thus creating many obstacles at their adoption. The purposes of the API is to exploit the advantages of linked data through a style of interaction that is familiar to web developers, thus helping them create data visualisations and applications.

The easy of use of an API is related both to the input of the API (API calls) and the out put. Regarding the input of the API there are mainly two design options: i) use a separate parameter for each required input and ii) model all the required input as a JSON object. The first option is post popular at existing APIs, while the second is more flexible and expressive. For example, it enables the expression of relations other than equality (e.g. greater than).


\begin{tabular}{|c|c| }\hline
\textbf{Separate parameters} & \textbf{Input as JSON}\\ \hline
\begin{minipage}[t]{2.2in}
 \begin{verbatim} 
GET /slice?dataset=home-care&
            gender=male         
\end{verbatim}
\end{minipage}
&
 \begin{minipage}[t]{2.3in}
\begin{verbatim} 
{"dataset": "home-care",
 "filter": {
   "gender": "male"
}}
\end{verbatim}
\end{minipage}\\\hline
\begin{minipage}[t]{2.2in}
 \begin{verbatim} 
-                    
\end{verbatim}
\end{minipage}
&
\begin{minipage}[t]{2.3in}
\begin{verbatim} 
{"dataset": "home-care",
 "filter": {
   "gender": "male"
   "age": { "greater-than": 50}
}}
\end{verbatim}
\end{minipage}\\\hline
\end{tabular}

Regarding the output of the API, JSON is a popular, easy to use format. Usually, applications and visualizations do not require an n-array/ tabular response; an array of observations is sufficient. In case that a tabular response is required, then it can easily be constructed from the observations. An excerpt of a JSON code that represents an array of observations is presented below.

\begin{verbatim} 
{ "observations": [ 
    {"Average Cost": "1182", 
     "Date": "1-1-2013", 
     "Day": "Tuesday", 
     "Number of crashes": "5",
     "Time": "No available time",
     "Total Cost": "5908"}, 
    {"Average Cost": "400",
     "Date": "1-1-2013",
     "Day": "Tuesday",
     "Number of crashes": "1",
     "Time": "24:00",
     "Total Cost": "400"}
]}
\end{verbatim}

Finally, JSON-LD representation can be used both at the input and output of the API format in order to represent linked data.

\subsection{Consistency}

Currently make linked data cubes have been published, however many of them adopt different publishing practices. The API should work on top of any of these data offering a uniform way to the data and hiding any data discrepancies. 


we want to standardise the API specification and try to get broad support for it, so that many data publishers can provide data in a compatible way, making tools interoperable. So the API has to be generic enough to be widely applicable.

Application profile???

\subsection{Performance}
It will be easier to support efficient caching of API responses than of arbitrary SPARQL queries


\subsection{Extensibility}
while our initial implementations are building on top of underlying RDF databases, other kinds of database could be used, enabling flexibility and innovation. 

Multilinguality


\subsection{Other}


as well as making data easier to consume, using the API as the main method of delivering machine readable extracts of data would remove or greatly reduce the need for data publishers to provide public SPARQL endpoints, so reducing cost and improving reliability of open online services

aggregations

\section{Implementation}\label{sec:impl}

\section{Conclusion}\label{sec:conclusion}


%\begin{acknowledgements}
%If you'd like to thank anyone, place your comments here
%and remove the percent signs.
%\end{acknowledgements}

\bibliographystyle{splncs03}


\bibliography{qbbibfile}   % name your BibTeX data base


\end{document}


